# The actual computational loop of this file was autogenerated by the
# sha256 ASM code generator sha256-586.pl and x86asm.pl from OpenSSL
# circa 2022-Dec (open source Apache 2.0 license).  The generated code
# was heavily modified to extract only the SHA extension accelerated
# version needed and then converted to use a modern 64-bit x86_64
# calling convention, exploit extra XMM registers on x86_64, interface
# with the existing library API and expose for library use.  Byte code
# was also replaced with actual Intel ops codes as modern assemblers
# should understand these op codes and it makes the code clearer.

# Make stack non-executable
.section .note.GNU-stack,"",@progbits

.text
.globl	fd_sha256_core_shaext
.type	fd_sha256_core_shaext,@function
.align	32
fd_sha256_core_shaext:
        # Note: At this point
        # rdi is state
        # rsi is block
        # rdx is block_cnt
        shlq    $6,%rdx
        addq    %rsi,%rdx                                  # rdx = block + block_cnt*64
        leaq    fd_sha256_core_shaext_Kmask+128(%rip),%rax # rax = K + 128, mask at rax + 128 = K + 256 (aligned)
        leaq    fd_sha256_core_endianness_swap_shuffle_mask+(%rip),%rcx # rcx = address of endianness shuffle mask
	movdqu	(%rdi),%xmm1
	movdqu	16(%rdi),%xmm2
	movdqa	(%rcx),%xmm7
	pshufd	$27,%xmm1,%xmm0
	pshufd	$177,%xmm1,%xmm1
	pshufd	$27,%xmm2,%xmm2
        palignr	$8,%xmm2,%xmm1
	punpcklqdq	%xmm0,%xmm2
.align	32
.L003loop_shaext:
	movdqu	(%rsi),%xmm3
	movdqu	16(%rsi),%xmm4
	movdqu	32(%rsi),%xmm5
        pshufb  %xmm7,%xmm3
	movdqu	48(%rsi),%xmm6
	movdqa	%xmm2,%xmm9
	movdqa	-128(%rax),%xmm0
	paddd	%xmm3,%xmm0
        pshufb  %xmm7,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	nop
	movdqa	%xmm1,%xmm8
	sha256rnds2	%xmm2,%xmm1
	movdqa	-112(%rax),%xmm0
	paddd	%xmm4,%xmm0
        pshufb  %xmm7,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	leaq	64(%rsi),%rsi
	sha256msg1	%xmm4,%xmm3
	sha256rnds2	%xmm2,%xmm1
	movdqa	-96(%rax),%xmm0
	paddd	%xmm5,%xmm0
        pshufb  %xmm7,%xmm6
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
        palignr	$4,%xmm5,%xmm7
	nop
	paddd	%xmm7,%xmm3
	sha256msg1	%xmm5,%xmm4
	sha256rnds2	%xmm2,%xmm1
	movdqa	-80(%rax),%xmm0
	paddd	%xmm6,%xmm0
	sha256msg2	%xmm6,%xmm3
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
        palignr	$4,%xmm6,%xmm7
	nop
	paddd	%xmm7,%xmm4
	sha256msg1	%xmm6,%xmm5
	sha256rnds2	%xmm2,%xmm1
	movdqa	-64(%rax),%xmm0
	paddd	%xmm3,%xmm0
	sha256msg2	%xmm3,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
        palignr	$4,%xmm3,%xmm7
	nop
	paddd	%xmm7,%xmm5
	sha256msg1	%xmm3,%xmm6
	sha256rnds2	%xmm2,%xmm1
	movdqa	-48(%rax),%xmm0
	paddd	%xmm4,%xmm0
	sha256msg2	%xmm4,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
        palignr	$4,%xmm4,%xmm7
	nop
	paddd	%xmm7,%xmm6
	sha256msg1	%xmm4,%xmm3
	sha256rnds2	%xmm2,%xmm1
	movdqa	-32(%rax),%xmm0
	paddd	%xmm5,%xmm0
	sha256msg2	%xmm5,%xmm6
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
        palignr	$4,%xmm5,%xmm7
	nop
	paddd	%xmm7,%xmm3
	sha256msg1	%xmm5,%xmm4
	sha256rnds2	%xmm2,%xmm1
	movdqa	-16(%rax),%xmm0
	paddd	%xmm6,%xmm0
	sha256msg2	%xmm6,%xmm3
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
        palignr	$4,%xmm6,%xmm7
	nop
	paddd	%xmm7,%xmm4
	sha256msg1	%xmm6,%xmm5
	sha256rnds2	%xmm2,%xmm1
	movdqa	(%rax),%xmm0
	paddd	%xmm3,%xmm0
	sha256msg2	%xmm3,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
        palignr	$4,%xmm3,%xmm7
	nop
	paddd	%xmm7,%xmm5
	sha256msg1	%xmm3,%xmm6
	sha256rnds2	%xmm2,%xmm1
	movdqa	16(%rax),%xmm0
	paddd	%xmm4,%xmm0
	sha256msg2	%xmm4,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
        palignr	$4,%xmm4,%xmm7
	nop
	paddd	%xmm7,%xmm6
	sha256msg1	%xmm4,%xmm3
	sha256rnds2	%xmm2,%xmm1
	movdqa	32(%rax),%xmm0
	paddd	%xmm5,%xmm0
	sha256msg2	%xmm5,%xmm6
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
        palignr	$4,%xmm5,%xmm7
	nop
	paddd	%xmm7,%xmm3
	sha256msg1	%xmm5,%xmm4
	sha256rnds2	%xmm2,%xmm1
	movdqa	48(%rax),%xmm0
	paddd	%xmm6,%xmm0
	sha256msg2	%xmm6,%xmm3
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
        palignr	$4,%xmm6,%xmm7
	nop
	paddd	%xmm7,%xmm4
	sha256msg1	%xmm6,%xmm5
	sha256rnds2	%xmm2,%xmm1
	movdqa	64(%rax),%xmm0
	paddd	%xmm3,%xmm0
	sha256msg2	%xmm3,%xmm4
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
        palignr	$4,%xmm3,%xmm7
	nop
	paddd	%xmm7,%xmm5
	sha256msg1	%xmm3,%xmm6
	sha256rnds2	%xmm2,%xmm1
	movdqa	80(%rax),%xmm0
	paddd	%xmm4,%xmm0
	sha256msg2	%xmm4,%xmm5
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
        palignr	$4,%xmm4,%xmm7
	sha256rnds2	%xmm2,%xmm1
	paddd	%xmm7,%xmm6
	movdqa	96(%rax),%xmm0
	paddd	%xmm5,%xmm0
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	sha256msg2	%xmm5,%xmm6
	movdqa	(%rcx),%xmm7
	sha256rnds2	%xmm2,%xmm1
	movdqa	112(%rax),%xmm0
	paddd	%xmm6,%xmm0
	nop
	sha256rnds2	%xmm1,%xmm2
	pshufd	$14,%xmm0,%xmm0
	cmpq	%rsi,%rdx
	nop
	sha256rnds2	%xmm2,%xmm1
	paddd	%xmm9,%xmm2
	paddd	%xmm8,%xmm1
	jnz	.L003loop_shaext
	pshufd	$177,%xmm2,%xmm2
	pshufd	$27,%xmm1,%xmm7
	pshufd	$177,%xmm1,%xmm1
	punpckhqdq	%xmm2,%xmm1
        palignr	$8,%xmm7,%xmm2
	movdqu	%xmm1,(%rdi)
	movdqu	%xmm2,16(%rdi)
	ret

.size	fd_sha256_core_shaext, .-fd_sha256_core_shaext
        .align 128
        .type   fd_sha256_core_shaext_Kmask, @object
        .size   fd_sha256_core_shaext_Kmask, 256
#include "fd_sha256_K.h"
fd_sha256_core_shaext_Kmask:
.long FD_SHA256_K

.size fd_sha256_core_endianness_swap_shuffle_mask, .-fd_sha256_core_endianness_swap_shuffle_mask
  .align 16
  .type fd_sha256_core_endianness_swap_shuffle_mask, @object
  .size fd_sha256_core_endianness_swap_shuffle_mask, 16
fd_sha256_core_endianness_swap_shuffle_mask:
.long 0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
